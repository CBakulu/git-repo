{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b2257a9",
      "metadata": {
        "id": "5b2257a9"
      },
      "source": [
        "# Tech Ethics\n",
        "\n",
        "## Introduction to Python\n",
        "\n",
        "Data Sciences Institute, University of Toronto\n",
        "\n",
        "Instructor: Kaylie Lau | TA: Salaar Liaqat \n",
        "\n",
        "November - December 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "xbocf-S-9ry7"
      },
      "id": "xbocf-S-9ry7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this class we will discuss tech ethics with a focus on identifying inequities that stem from racism, bias, and discrimination. It should be noted that tech ethics includes a vast number of topics, and while the structure of this course does not allow for in-detail conversations, we hope that this class will be a helpful entry point for your continued exploration."
      ],
      "metadata": {
        "id": "OWbjwcV6-ELo"
      },
      "id": "OWbjwcV6-ELo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithmic Bias\n",
        "\n",
        "- Systematic and repeatable errors in a computer system that create unfair outcomes.\n",
        "- Can emerge from the design of an algorithm, unintended or unanticipated use of an algorithm, or decisions relating to the way data is coded, collected, selected, or used to train an algorithm."
      ],
      "metadata": {
        "id": "ZXVAof0A1YHs"
      },
      "id": "ZXVAof0A1YHs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\n",
        "- [2018 study conducted by Joy Buolamwini and Timnit Gebru](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)\n",
        "- Found that automated facial analysis algorithms can discriminate based on classes like race and gender.\n",
        "- Evaluated algorithms used by Microsoft, IBM, and Face++, and found that the models had a maximum error rate of 34.7% for darker-skinned females compared with 0.8% for lighter-skinned males."
      ],
      "metadata": {
        "id": "zOoDAurhhNsx"
      },
      "id": "zOoDAurhhNsx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Racial disparities in automated speech recognition\n",
        "- [2020 study conducted by Allison Koenecke et al.](https://www.pnas.org/doi/abs/10.1073/pnas.1915768117)\n",
        "- Found that the performance of automated speech recognition systems (i.e., systems that use machine-learning algorithms to convert spoken language to text) can have large racial disparities.\n",
        "- Evaluated systems developed by Amazon, Apple, Google, IBM, and Microsoft, and found that the systems had an average word error rate of 35% for black speakers compared with 19% for white speakers."
      ],
      "metadata": {
        "id": "Rvsqb9kEq2U1"
      },
      "id": "Rvsqb9kEq2U1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Risk Assessment Tools on Trial: AI Systems Go?\n",
        "- [2022 article by Neha Chugh](https://ieeexplore.ieee.org/document/9893513/authors#authors)\n",
        "- In the United States, AI risk assessments are being increasingly used as tools to calculate a defendant’s risk of reoffending.\n",
        "- In 2016, ProPublica found that the COMPAS risk assessment tool used in Broward County, FL, USA, produced unreliable predictions and significant racial disparities between White and Black defendants.\n",
        "- While the use of AI risk assessments has not yet been implemented in Canadian courtrooms, it is clear that there are a number of issues that need to be addressed."
      ],
      "metadata": {
        "id": "HRsXV0mEFhX-"
      },
      "id": "HRsXV0mEFhX-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indigenous data sovereignty\n",
        "\n",
        "Indigenous data sovereignty principles assert that First Nations, Inuit, and Métis have collective sovereign rights and the fundamental authority to own and govern their data, regardless of where their data is housed."
      ],
      "metadata": {
        "id": "9MuGrWLhPBU2"
      },
      "id": "9MuGrWLhPBU2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genomic Data\n",
        "- [Data sovereignty in genomics and medical research](https://www.nature.com/articles/s42256-022-00578-1) and [Federated learning and Indigenous genomic data sovereignty](https://www.nature.com/articles/s42256-022-00551-y)\n",
        "- Need to include diverse populations in genomics datasets but open data sharing is not always in the best interest of certain communities.\n",
        "- Indigenous people are under-represented in genomics datasets, but are understandably apprehensive about contributing their data owing to a history of mistrust and scepticism from certain interactions with medical researchers.\n",
        "- Data sovereignty is a way for Indigenous communities to control, or take back control, of their personal data."
      ],
      "metadata": {
        "id": "DO3eEnagoGdK"
      },
      "id": "DO3eEnagoGdK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Nations Information Governance Centre (FNIGC)\n",
        "\n",
        "\n",
        "\n",
        "*   Data gathering initiatives on- reserve and in Northern First Nations communities, such as:\n",
        "  * First Nations Regional Health Survey (FNRHS)\n",
        "  * First Nations Regional Early Childhood\n",
        "  * Education and Employment Survey (FNREEES)\n",
        "  * First Nations Labour and Employment Development (FNLED) survey\n",
        "  * First Nations Community Survey (FNCS)\n",
        "*   First Nations principles of OCAP® (stands for ownership, control, access, and possession), which are a set of standards for how to conduct research with First Nations\n",
        "  * Establish how First Nations data should be collected, protected, used, or shared.\n",
        "  * First Nations control data collection processes in their communities and they also own, protect, and control how their information is used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N243cov9U-yb"
      },
      "id": "N243cov9U-yb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Canadian Institute for Health Information (CIHI)\n",
        "\n",
        "* CIHI has a policy that requires that any request for Indigenous-identifiable data be accompanied by approvals\n",
        "from appropriate Indigenous authorities (i.e., First Nations, Inuit and/or Métis governments, communities and/or organizations).\n",
        "* They are also working on releasing standards for collecting race-based and Indigenous identity data in health systems, along with guidance on their use.\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZcWgfeTxGQo"
      },
      "id": "9ZcWgfeTxGQo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion Questions\n",
        "- Think about your current work. Why do you need to think about ethics?\n",
        "- Think about people in your life (family, friends, co-workers). How ethical/non-ethical decisions could impact them?\n",
        "- How do we as data scientists combine quantitative approaches with other ways of knowing?"
      ],
      "metadata": {
        "id": "c324TR4gVwyU"
      },
      "id": "c324TR4gVwyU"
    },
    {
      "cell_type": "markdown",
      "id": "b17bd03e",
      "metadata": {
        "id": "b17bd03e"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Buolamwini, J., & Gebru, T. (2018, January). Gender shades: Intersectional accuracy disparities in commercial gender classification. In *Conference on fairness, accountability and transparency *(pp. 77-91). PMLR.\n",
        "Chicago\t\n",
        "- Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., ... & Goel, S. (2020). Racial disparities in automated speech recognition. *Proceedings of the National Academy of Sciences*, 117(14), 7684-7689.\n",
        "- Chugh, N. (2022). Risk Assessment Tools on Trial: AI Systems Go?. *IEEE Technology and Society Magazine*, 41(3), 50-57.\n",
        "Chicago\t\n",
        "- Data sovereignty in genomics and medical research. *Nat Mach Intell* 4, 905–906 (2022).\n",
        "- Boscarino, N., Cartwright, R. A., Fox, K., & Tsosie, K. S. (2022). Federated learning and Indigenous genomic data sovereignty. *Nature Machine Intelligence*, 1-3.\n",
        "- First Nations Information Governance Centre (FNIGC). https://fnigc.ca\n",
        "- Canadian Institute for Health Information. (2022, March 17). *Race-based and Indigenous identity data*. https://www.cihi.ca/en/race-based-and-indigenous-identity-data\n",
        "- Canadian Institute for Health Information. (2020, August). *Toward Respectful Governance of First Nations, Inuit and Métis Data Housed at CIHI*. https://www.cihi.ca/sites/default/files/document/path-toward-respectful-governance-fnim-2020-report-en.pdf"
      ],
      "metadata": {
        "id": "C_hqSl6jRsnK"
      },
      "id": "C_hqSl6jRsnK"
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xbocf-S-9ry7",
        "ZXVAof0A1YHs"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}